# 13 AI驯兽师：神经网络调教综述

在未来的AI时代，“手工程序”将变得越发稀有，而基于通用AI程序，通过大数据“习得”而生的程序，会无所不在。到那时，程序员将光荣卸任，取而代之的是一个新职业物种：他们无需像程序员那样了解所有细节，而是关注数据的获取和筛选、模型的训练和调教。他们是**AI驯兽师**。

在过去的两场人机围棋旷世之战中，替AlphaGo执棋的黄士杰就是AI驯兽师的先驱：一个业余六段棋手作为首席工程师打造出AlphaGo，完胜人类专业九段，至此再无人类对手。

![驯兽师](img/2017-13-train.jpg)

前面MNIST识别的实现，选用的各种参数值看似天经地义，一帆风顺，实则都是前人的经验，而真实情况下的调教过程，必定充满了过去不曾留意过的种种困难和不确定性。还记得在[12 TensorFlow构建3层NN玩转MNIST](./12-TensorFlow构建3层NN实现手写体数字识别.md)中，一个不小心令权重和偏置初始化为0，导致了识别率连60%都无法逾越的结果。

本篇先对神经网络的调教做一个总览，作为“驯兽”的简要指南。

## 调教的几个层面

神经网络可调的选项实在太多了，也并不简单，我简单把它梳理为5个层面。

### 第一个层面：网络架构

网络的架构是在训练之前就需要确定的，包括：

- 输入层神经元数量；
- 输出层神经元数量；
- 隐藏层的数量，以及各隐藏层神经元的数量；
- 隐藏层的种类：全连接层（FC），批标准化层（BN），卷积层（Convolutional），池化层（Pooling），Inception Module，Res Module；
- 隐藏层神经元激活函数的形式：Sigmoid， Tanh，ReLU，Leaky ReLU，Swish；
- 输出层神经元激活函数的形式：Sigmoid，Softmax；

理论上，网络架构的规模越大，对复杂模型的表达就越充分。可随之而来的副作用也相当明显：训练难度相应增大，同时容易发生过拟合。此外，由于基于梯度下降的神经网络算法自身的局限、计算量等一系列因素，当全连接网络的规模增加到一定程度时（尤其是深度规模），模型性能的提升会越发困难。

所以网络规模并不是越大越好，要根据问题的规模以及数据量的规模来综合考虑。

### 第二个层面：超参数

一旦网络架构定义完毕，那么除了网络自身的可训练参数之外，其余的参数都可以被认为是超参数，包括：

- epoch，迭代数量；
- mini batch；
- learning rate，学习率；
- lambda，正则化参数（如果损失函数进行了L1或L2正则化）；
- γ，β，批标准化参数（如果网络中包含BN层）；
- 卷积核数量、尺寸、步长Stride、Padding数量（如果包含卷积层）；

### 第三个层面：权重和偏置初始化

开始学习之前，权重和偏置的数值分布状态，也会很大的影响到模型的精度，以及学习的速度。常见的初始化方式：

- 初始化为0；
- 初始化均值为0，标准差为1；
- 初始化均值为0，标准差为1/&radic;n；
- Xavier/He初始化方法；

其中第一种初始化为0，就遇到了之前提到的“60%识别率”的严重状况。

第二种是我们目前的已有MNIST识别实现所采用的初始化方法。相较于第二种方式，后两种会明显改善学习速度，后面的文章还会具体讲。

### 第四个层面：数据使用

用于模型学习的数据当然是越大越好，可现实中它总是稀缺而昂贵。在这种情况下就需要合理的划分和使用数据：

- 训练数据的数量；
- 验证数据的数量；
- 测试数据的数量；
- 数据的人为扩展；

训练集、验证集和测试集的划分方式我们已经了解了（参考[11 74行Python实现手写体数字识别](./11-74行Python实现手写体数字识别.md)）。这里简单说下数据的扩展。

以图像数据为例。我们知道，只要把原图像整体挪动1个像素，就会得到一张全新的图像，由于图像的大部分像素的相对位置保持不变，所以其包含的语义信息仍然是完整无缺的。这样就可以在现有的数据基础上，人为产生更多的新数据。方法不限于平移，还可以做旋转、镜像、扭曲、添加噪音等等，以此来训练并提高模型的泛化能力。

### 第五个层面：最优化算法

即便是处于训练算法最外层的最优化算法框架，也可以被替换：

- 损失函数的形式：均方误差（MSE），交叉熵（Cross Entrop），对数似然（Log Likelihood）；
- 最优化算法框架：随机梯度下降（SDG），Hessian，动量更新（Momentum），NAG（ Nesterov Accelerated  Gradient）；
- 自适应学习率算法：AdaGrad， RMSProp，Adam；
- 基于全矩阵法的小批量数据（mini batch）反向传播；

## 调教目标和策略

调教神经网络的终极目标，狭义的说就是测试集上的识别精度。

尽管目标明确，但是整个训练过程只能间接的影响它——模型学习的数据是来自训练集，而测试集的识别精度要依靠**模型的泛化能力**来支撑。

提高模型泛化能力的切入点，并不是盲目的去尝试调整上面所有层面的选项，而通常是从着手改善问题开始的。未经优化的神经网络，通常都存在以下两个问题：

- 学习缓慢；
- 过拟合。

## 小结

本篇从整体上分析了神经网络调教的几个层面，以及调教的目标和策略。每当需要优化神经网络时，可以把它当做一份check list。